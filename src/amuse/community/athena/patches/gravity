Index: athena/src/athena4.1/src/gravity/bvals_grav.c
===================================================================
--- athena.orig/src/athena4.1/src/gravity/bvals_grav.c	2010-03-01 16:14:02.000000000 +0100
+++ athena/src/athena4.1/src/gravity/bvals_grav.c	2011-02-09 17:03:36.000000000 +0100
@@ -25,6 +25,7 @@
 #ifdef SELF_GRAVITY
 
 #ifdef MPI_PARALLEL
+#include "../globals.h"
 /* MPI send and receive buffers */
 static double **send_buf = NULL, **recv_buf = NULL;
 static MPI_Request *recv_rq, *send_rq;
@@ -103,8 +104,8 @@
   if (pGrid->Nx[0] > 1){
 
 #ifdef MPI_PARALLEL
-    cnt2 = pGrid->Nx2 > 1 ? pGrid->Nx2 + 1 : 1;
-    cnt3 = pGrid->Nx3 > 1 ? pGrid->Nx3 + 1 : 1;
+    cnt2 = pGrid->Nx[1] > 1 ? pGrid->Nx[1] + 1 : 1;
+    cnt3 = pGrid->Nx[2] > 1 ? pGrid->Nx[2]+ 1 : 1;
     cnt = nghost*cnt2*cnt3;
 
 /* MPI blocks to both left and right */
@@ -117,11 +118,11 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data L and R */
-      pack_ix1(pGrid);
+      pack_Phi_ix1(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx1_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
-      pack_ox1(pGrid);
+      pack_Phi_ox1(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx1_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -130,11 +131,11 @@
 
       /* check non-blocking receives and unpack data in any order. */
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix1(pGrid);
-      if (mIndex == 1) unpack_ox1(pGrid);
+      if (mIndex == 0) unpack_Phi_ix1(pGrid);
+      if (mIndex == 1) unpack_Phi_ox1(pGrid);
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix1(pGrid);
-      if (mIndex == 1) unpack_ox1(pGrid);
+      if (mIndex == 0) unpack_Phi_ix1(pGrid);
+      if (mIndex == 1) unpack_Phi_ox1(pGrid);
 
     }
 
@@ -146,7 +147,7 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data R */
-      pack_ox1(pGrid);
+      pack_Phi_ox1(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx1_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -158,7 +159,7 @@
 
       /* wait on non-blocking receive from R and unpack data */
       ierr = MPI_Wait(&(recv_rq[1]), MPI_STATUS_IGNORE);
-      unpack_ox1(pGrid);
+      unpack_Phi_ox1(pGrid);
 
     }
 
@@ -170,7 +171,7 @@
         pD->Comm_Domain, &(recv_rq[0]));
 
       /* pack and send data L */
-      pack_ix1(pGrid);
+      pack_Phi_ix1(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx1_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
@@ -182,7 +183,7 @@
 
       /* wait on non-blocking receive from L and unpack data */
       ierr = MPI_Wait(&(recv_rq[0]), MPI_STATUS_IGNORE);
-      unpack_ix1(pGrid);
+      unpack_Phi_ix1(pGrid);
 
     }
 #endif /* MPI_PARALLEL */
@@ -201,8 +202,8 @@
   if (pGrid->Nx[1] > 1){
 
 #ifdef MPI_PARALLEL
-    cnt1 = pGrid->Nx1 > 1 ? pGrid->Nx1 + 2*nghost : 1;
-    cnt3 = pGrid->Nx3 > 1 ? pGrid->Nx3 + 1 : 1;
+    cnt1 = pGrid->Nx[0] > 1 ? pGrid->Nx[0] + 2*nghost : 1;
+    cnt3 = pGrid->Nx[2] > 1 ? pGrid->Nx[2] + 1 : 1;
     cnt = nghost*cnt1*cnt3;
 
 /* MPI blocks to both left and right */
@@ -215,11 +216,11 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data L and R */
-      pack_ix2(pGrid);
+      pack_Phi_ix2(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx2_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
-      pack_ox2(pGrid);
+      pack_Phi_ox2(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx2_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -228,11 +229,11 @@
 
       /* check non-blocking receives and unpack data in any order. */
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix2(pGrid);
-      if (mIndex == 1) unpack_ox2(pGrid);
+      if (mIndex == 0) unpack_Phi_ix2(pGrid);
+      if (mIndex == 1) unpack_Phi_ox2(pGrid);
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix2(pGrid);
-      if (mIndex == 1) unpack_ox2(pGrid);
+      if (mIndex == 0) unpack_Phi_ix2(pGrid);
+      if (mIndex == 1) unpack_Phi_ox2(pGrid);
 
     }
 
@@ -244,7 +245,7 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data R */
-      pack_ox2(pGrid);
+      pack_Phi_ox2(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx2_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -256,7 +257,7 @@
 
       /* wait on non-blocking receive from R and unpack data */
       ierr = MPI_Wait(&(recv_rq[1]), MPI_STATUS_IGNORE);
-      unpack_ox2(pGrid);
+      unpack_Phi_ox2(pGrid);
 
     }
 
@@ -268,7 +269,7 @@
         pD->Comm_Domain, &(recv_rq[0]));
 
       /* pack and send data L */
-      pack_ix2(pGrid);
+      pack_Phi_ix2(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx2_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
@@ -280,7 +281,7 @@
 
       /* wait on non-blocking receive from L and unpack data */
       ierr = MPI_Wait(&(recv_rq[0]), MPI_STATUS_IGNORE);
-      unpack_ix2(pGrid);
+      unpack_Phi_ix2(pGrid);
 
     }
 #endif /* MPI_PARALLEL */
@@ -297,7 +298,7 @@
     if (my_iproc == 0) {
       ShearingSheet_grav_ix1(pGrid, pDomain);
     }
-    if (my_iproc == (pDomain->NGrid_x1-1)) {
+    if (my_iproc == (pDomain->NGrid[0]-1)) {
       ShearingSheet_grav_ox1(pGrid, pDomain);
     }
 #endif
@@ -310,8 +311,8 @@
   if (pGrid->Nx[2] > 1){
 
 #ifdef MPI_PARALLEL
-    cnt1 = pGrid->Nx1 > 1 ? pGrid->Nx1 + 2*nghost : 1;
-    cnt2 = pGrid->Nx2 > 1 ? pGrid->Nx2 + 2*nghost : 1;
+    cnt1 = pGrid->Nx[0] > 1 ? pGrid->Nx[0] + 2*nghost : 1;
+    cnt2 = pGrid->Nx[1] > 1 ? pGrid->Nx[1] + 2*nghost : 1;
     cnt = nghost*cnt1*cnt2;
 
 /* MPI blocks to both left and right */
@@ -324,11 +325,11 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data L and R */
-      pack_ix3(pGrid);
+      pack_Phi_ix3(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx3_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
-      pack_ox3(pGrid);
+      pack_Phi_ox3(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx3_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -337,11 +338,11 @@
 
       /* check non-blocking receives and unpack data in any order. */
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix3(pGrid);
-      if (mIndex == 1) unpack_ox3(pGrid);
+      if (mIndex == 0) unpack_Phi_ix3(pGrid);
+      if (mIndex == 1) unpack_Phi_ox3(pGrid);
       ierr = MPI_Waitany(2,recv_rq,&mIndex,MPI_STATUS_IGNORE);
-      if (mIndex == 0) unpack_ix3(pGrid);
-      if (mIndex == 1) unpack_ox3(pGrid);
+      if (mIndex == 0) unpack_Phi_ix3(pGrid);
+      if (mIndex == 1) unpack_Phi_ox3(pGrid);
 
     }
 
@@ -353,7 +354,7 @@
         pD->Comm_Domain, &(recv_rq[1]));
 
       /* pack and send data R */
-      pack_ox3(pGrid);
+      pack_Phi_ox3(pGrid);
       ierr = MPI_Isend(send_buf[1], cnt, MPI_DOUBLE, pGrid->rx3_id, LtoR_tag,
         pD->Comm_Domain, &(send_rq[1]));
 
@@ -365,7 +366,7 @@
 
       /* wait on non-blocking receive from R and unpack data */
       ierr = MPI_Wait(&(recv_rq[1]), MPI_STATUS_IGNORE);
-      unpack_ox3(pGrid);
+      unpack_Phi_ox3(pGrid);
 
     }
 
@@ -377,7 +378,7 @@
         pD->Comm_Domain, &(recv_rq[0]));
 
       /* pack and send data L */
-      pack_ix3(pGrid);
+      pack_Phi_ix3(pGrid);
       ierr = MPI_Isend(send_buf[0], cnt, MPI_DOUBLE, pGrid->lx3_id, RtoL_tag,
         pD->Comm_Domain, &(send_rq[0]));
 
@@ -389,7 +390,7 @@
 
       /* wait on non-blocking receive from L and unpack data */
       ierr = MPI_Wait(&(recv_rq[0]), MPI_STATUS_IGNORE);
-      unpack_ix3(pGrid);
+      unpack_Phi_ix3(pGrid);
     }
 #endif /* MPI_PARALLEL */
 
@@ -460,7 +461,7 @@
           case 4: /* Periodic */
             ix1_GBCFun = periodic_Phi_ix1;
 #ifdef MPI_PARALLEL
-            if(pG->lx1_id < 0 && pD->NGrid_x1 > 1){
+            if(pG->lx1_id < 0 && pD->NGrid[0] > 1){
               pG->lx1_id = pD->GData[myN][myM][pD->NGrid[0]-1].ID_Comm_Domain;
 	    }
 #endif /* MPI_PARALLEL */
@@ -500,7 +501,7 @@
           case 4: /* Periodic */
             ox1_GBCFun = periodic_Phi_ox1;
 #ifdef MPI_PARALLEL
-            if(pG->rx1_id < 0 && pD->NGrid_x1 > 1){
+            if(pG->rx1_id < 0 && pD->NGrid[0] > 1){
               pG->rx1_id = pD->GData[myN][myM][0].ID_Comm_Domain;
             }
 #endif /* MPI_PARALLEL */
@@ -545,7 +546,7 @@
           case 4: /* Periodic */
             ix2_GBCFun = periodic_Phi_ix2;
 #ifdef MPI_PARALLEL
-            if(pG->lx2_id < 0 && pD->NGrid_x2 > 1){
+            if(pG->lx2_id < 0 && pD->NGrid[1] > 1){
               pG->lx2_id = pD->GData[myN][pD->NGrid[1]-1][myL].ID_Comm_Domain;
             }
 #endif /* MPI_PARALLEL */
@@ -585,7 +586,7 @@
           case 4: /* Periodic */
             ox2_GBCFun = periodic_Phi_ox2;
 #ifdef MPI_PARALLEL
-            if(pG->rx2_id < 0 && pD->NGrid_x2 > 1){
+            if(pG->rx2_id < 0 && pD->NGrid[1] > 1){
               pG->rx2_id = pD->GData[myN][0][myL].ID_Comm_Domain;
             }
 #endif /* MPI_PARALLEL */
@@ -630,7 +631,7 @@
           case 4: /* Periodic */
             ix3_GBCFun = periodic_Phi_ix3;
 #ifdef MPI_PARALLEL
-            if(pG->lx3_id < 0 && pD->NGrid_x3 > 1){
+            if(pG->lx3_id < 0 && pD->NGrid[2] > 1){
               pG->lx3_id = pD->GData[pD->NGrid[2]-1][myM][myL].ID_Comm_Domain;
             }
 #endif /* MPI_PARALLEL */
@@ -670,7 +671,7 @@
           case 4: /* Periodic */
             ox3_GBCFun = periodic_Phi_ox3;
 #ifdef MPI_PARALLEL
-            if(pG->rx3_id < 0 && pD->NGrid_x3 > 1){
+            if(pG->rx3_id < 0 && pD->NGrid[2] > 1){
               pG->rx3_id = pD->GData[0][myM][myL].ID_Comm_Domain;
             }
 #endif /* MPI_PARALLEL */
@@ -1078,7 +1079,7 @@
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Inner x1 boundary */
 
-static void pack_Phi_ix1(GridS *pG)
+static void pack_Phi_ix1(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1090,7 +1091,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js; j<=je; j++){
       for (i=is; i<=is+(nghost-1); i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
@@ -1101,7 +1102,7 @@
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Outer x1 boundary */
 
-static void pack_Phi_ox1(GridS *pG)
+static void pack_Phi_ox1(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1113,7 +1114,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js; j<=je; j++){
       for (i=ie-(nghost-1); i<=ie; i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
@@ -1124,7 +1125,7 @@
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Inner x2 boundary */
 
-static void pack_Phi_ix2(GridS *pG)
+static void pack_Phi_ix2(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1136,7 +1137,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js; j<=js+(nghost-1); j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
@@ -1147,7 +1148,7 @@
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Outer x2 boundary */
 
-static void pack_Phi_ox2(GridS *pG)
+static void pack_Phi_ox2(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1160,7 +1161,7 @@
   for (k=ks; k<=ke; k++){
     for (j=je-(nghost-1); j<=je; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
@@ -1171,12 +1172,13 @@
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Inner x3 boundary */
 
-static void pack_Phi_ix3(GridS *pG)
+static void pack_Phi_ix3(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
   int ks = pGrid->ks, ke = pGrid->ke;
   int i,j,k;
+  int ierr, cnt;
   double *pSnd = send_buf[0];
 
 /* Pack only Phi into send buffer */
@@ -1184,23 +1186,23 @@
   for (k=ks; k<=ks+(nghost-1); k++){
     for (j=js-nghost; j<=je+nghost; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
 
 /* send contents of buffer to the neighboring grid on L-x3 */
 
-  ierr = MPI_Send(send_buf, cnt, MPI_DOUBLE, pG->lx3_id,
+  /*ierr = MPI_Send(send_buf, cnt, MPI_DOUBLE, pGrid->lx3_id,
 		  boundary_cells_tag, MPI_COMM_WORLD);
-
+    */
   return;
 }
 
 /*----------------------------------------------------------------------------*/
 /* PACK boundary conditions for MPI_Isend, Outer x3 boundary */
 
-static void pack_Phi_ox3(GridS *pG)
+static void pack_Phi_ox3(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1213,7 +1215,7 @@
   for (k=ke-(nghost-1); k<=ke; k++){
     for (j=js-nghost; j<=je+nghost; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        *(pSnd++) = pG->Phi[k][j][i];
+        *(pSnd++) = pGrid->Phi[k][j][i];
       }
     }
   }
@@ -1224,7 +1226,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Inner x1 boundary */
 
-static void unpack_Phi_ix1(GridS *pG)
+static void unpack_Phi_ix1(GridS *pGrid)
 {
   int is = pGrid->is;
   int js = pGrid->js, je = pGrid->je;
@@ -1237,7 +1239,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js; j<=js; j++){
       for (i=is-nghost; i<=is-1; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
@@ -1248,7 +1250,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Outer x1 boundary */
 
-static void unpack_Phi_ox1(GridS *pG)
+static void unpack_Phi_ox1(GridS *pGrid)
 {
   int ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1261,7 +1263,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js; j<=je; j++){
       for (i=ie+1; i<=ie+nghost; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
@@ -1272,7 +1274,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Inner x2 boundary */
 
-static void unpack_Phi_ix2(GridS *pG)
+static void unpack_Phi_ix2(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js;
@@ -1285,7 +1287,7 @@
   for (k=ks; k<=ke; k++){
     for (j=js-nghost; j<=js-1; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
@@ -1296,7 +1298,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Outer x2 boundary */
 
-static void unpack_Phi_ox2(GridS *pG)
+static void unpack_Phi_ox2(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int je = pGrid->je;
@@ -1309,7 +1311,7 @@
   for (k=ks; k<=ke; k++){
     for (j=je+1; j<=je+nghost; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
@@ -1320,7 +1322,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Inner x3 boundary */
 
-static void unpack_Phi_ix3(GridS *pG)
+static void unpack_Phi_ix3(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1333,7 +1335,7 @@
   for (k=ks-nghost; k<=ks-1; k++){
     for (j=js-nghost; j<=je+nghost; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
@@ -1344,7 +1346,7 @@
 /*----------------------------------------------------------------------------*/
 /* UNPACK boundary conditions after MPI_Irecv, Outer x3 boundary */
 
-static void unpack_Phi_ox3(GridS *pG)
+static void unpack_Phi_ox3(GridS *pGrid)
 {
   int is = pGrid->is, ie = pGrid->ie;
   int js = pGrid->js, je = pGrid->je;
@@ -1357,7 +1359,7 @@
   for (k=ke+1; k<=ke+nghost; k++){
     for (j=js-nghost; j<=je+nghost; j++){
       for (i=is-nghost; i<=ie+nghost; i++){
-        pG->Phi[k][j][i] = *(pRcv++);
+        pGrid->Phi[k][j][i] = *(pRcv++);
       }
     }
   }
